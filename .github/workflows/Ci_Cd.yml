name: Flight Booking CICD

on:
  push:
    branches:
      - dev
      - main #This includes cases when PR is merged into main Via pull request from dev to main remember that

jobs:
  upload-to-dev:
    if: github.ref == 'refs/heads/dev'    
    # This job will only run if the push event is to the 'dev' branch
    # If the push event happens to be for the 'main' branch or any other branch, the job will not run
    
    runs-on: ubuntu-latest

    steps:
      # Checkout the repository
      - name: Checkout Code
        uses: actions/checkout@v3

      # Authenticate to GCP
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }} 
          # This is Stored in Github secrets. We have copied it from IAM -> Service Accounts -> Download The Key 
          # GO to Github.Open the repo where the code is . GO to settings . Secrets and Variables. add The content of the KEY Which is in JSON format.

      # Setup Google Cloud SDK
      - name: Setup Google Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }} # This is Stored in Github secrets. We have copied it from Project name in the top LEFT and center on Gcloud console 

      # Upload `variables.json` to Composer bucket
      - name: Upload Variables JSON to GCS
        run: |
          gsutil cp variables/dev/variables.json gs://us-central1-airflow-dev-44b39ca5-bucket/data/dev/variables.json

      # Import Variables into Airflow-DEV
      - name: Import Variables into Airflow-DEV
        run: |
          gcloud composer environments run airflow-dev \
            --location us-central1 \
            variables import -- /home/airflow/gcs/data/dev/variables.json

      # Sync Spark job to GCS
      - name: Upload Spark Job to GCS
        run: |
          gsutil cp spark_job/spark_job.py gs://airflow-projetc-flight/airflow-project/spark-job/

      # Sync Airflow DAG to Airflow DEV Composer
      - name: Upload Airflow DAG to DEV
        run: |
          gcloud composer environments storage dags import \
            --environment airflow-dev \
            --location us-central1 \
            --source airflow_job/airflow_job.py

  upload-to-prod:
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest

    steps:
      # Checkout the repository
      - name: Checkout Code
        uses: actions/checkout@v3

      # Authenticate to GCP
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      # Setup Google Cloud SDK
      - name: Setup Google Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      # Upload `variables.json` to Composer bucket
      - name: Upload Variables JSON to GCS
        run: |
          gsutil cp variables/prod/variables.json gs://us-central1-airflow-prod-5fd790a9-bucket/data/prod/variables.json

      # Import Variables into Airflow-PROD
      - name: Import Variables into Airflow-PROD
        run: |
          gcloud composer environments run airflow-prod \
            --location us-central1 \
            variables import -- /home/airflow/gcs/data/prod/variables.json

      # Sync Spark job to GCS
      - name: Upload Spark Job to GCS
        run: |
          gsutil cp spark_job/spark_job.py gs://airflow-projetc-flight/airflow-project/spark-job/

      # Sync Airflow DAG to Airflow PROD Composer
      - name: Upload Airflow DAG to PROD
        run: |
          gcloud composer environments storage dags import \
            --environment airflow-prod \
            --location us-central1 \
            --source airflow_job/airflow_job.py